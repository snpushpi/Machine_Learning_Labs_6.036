{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MIT 6.036 HW04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpushpi/MIT_machine_learning/blob/master/Copy_of_MIT_6_036_HW04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmv3jlgr4_Ji",
        "colab_type": "text"
      },
      "source": [
        "# MIT 6.036 Spring 2020: Homework 4\n",
        "This homework does not include provided Python code. Instead, we\n",
        "encourage you to write your own code to help you answer some of these\n",
        "problems, and/or test and debug the code components we do ask for.\n",
        "Some of the problems below are simple enough that hand calculation\n",
        "should be possible; your hand solutions can serve as test cases for\n",
        "your code.  You may also find that including utilities written in\n",
        "previous labs (like a `sd` or signed distance function) will be\n",
        "helpful, as you build up additional functions and utilities for\n",
        "calculation of margins, different loss functions, gradients, and other\n",
        "functions needed for margin maximization and gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N622h8-D5i-M",
        "colab_type": "code",
        "outputId": "48983a0d-bc09-4bd2-e5fb-98afd1d4a305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!rm -rf code_and_data_for_hw4*\n",
        "!rm -rf mnist\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw04/code_and_data_for_hw04.zip\n",
        "!unzip code_and_data_for_hw04.zip\n",
        "!mv code_and_data_for_hw04/* .\n",
        "  \n",
        "from code_for_hw04 import *\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_and_data_for_hw04.zip\n",
            "  inflating: code_and_data_for_hw04/code_for_hw04.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUS51a8m5rEI",
        "colab_type": "text"
      },
      "source": [
        "## 3) Implementing gradient descent\n",
        "In this section we will implement generic versions of gradient descent and apply these to the logistic regression objective.\n",
        "\n",
        "<b>Note: </b> If you need a refresher on gradient descent,\n",
        "you may want to reference\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Fall/courseware/Week4/gradient_descent/5\">this week's notes</a>.\n",
        "\n",
        "### 3.1) Implementing Gradient Descent\n",
        "We want to find the $x$ that minimizes the value of the *objective\n",
        "function* $f(x)$, for an arbitrary scalar function $f$.  The function\n",
        "$f$ will be implemented as a Python function of one argument, that\n",
        "will be a numpy column vector.  For efficiency, we will work with\n",
        "Python functions that return not just the value of $f$ at $f(x)$ but\n",
        "also return the gradient vector at $x$, that is, $\\nabla_x f(x)$.\n",
        "\n",
        "We will now implement a generic gradient descent function, `gd`, that\n",
        "has the following input arguments:\n",
        "\n",
        "* `f`: a function whose input is an `x`, a column vector, and\n",
        "  returns a scalar.\n",
        "* `df`: a function whose input is an `x`, a column vector, and\n",
        "  returns a column vector representing the gradient of `f` at `x`.\n",
        "* `x0`: an initial value of $x$, `x0`, which is a column vector.\n",
        "* `step_size_fn`: a function that is given the iteration index (an\n",
        "  integer) and returns a step size.\n",
        "* `num_steps`: the number of iterations to perform\n",
        "\n",
        "Our function `gd` returns a tuple:\n",
        "\n",
        "* x: the value at the final step\n",
        "* fx: the value of f(x) at the final step\n",
        "\n",
        "**Hint:** This is a short function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s03NFuxG6kvt",
        "colab_type": "text"
      },
      "source": [
        "The main function to implement is `gd`, defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNsLE3bg6jt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gd(f, df, x0, step_size_fn, num_steps):\n",
        "    X = x0\n",
        "    for i in range(num_steps):\n",
        "        step_size = step_size_fn(i)\n",
        "        X = X- step_size*df(X)\n",
        "    return X,f(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXu60n-H5_Hz",
        "colab_type": "text"
      },
      "source": [
        "To evaluate results, we also use a simple `package_ans` function,\n",
        "which checks the final `x` and `fx` values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN_XbacQ6Rue",
        "colab_type": "text"
      },
      "source": [
        "The test cases are provided below, but you should feel free (and are encouraged!) to write more of your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJcClaqN4nE6",
        "colab_type": "code",
        "outputId": "3b39be87-f640-4142-db76-c838af2ed96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_gd(gd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1:\n",
            "Passed!\n",
            "Test 2:\n",
            "Passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbuSt5hY645k",
        "colab_type": "text"
      },
      "source": [
        "### 3.2) Numerical Gradient\n",
        "Getting the analytic gradient correct for complicated functions is\n",
        "tricky.  A very handy method of verifying the analytic gradient or\n",
        "even substituting for it is to estimate the gradient at a point by\n",
        "means of *finite differences*.\n",
        "\n",
        "Assume that we are given a function $f(x)$ that takes a column vector\n",
        "as its argument and returns a scalar value.  In gradient descent, we\n",
        "will want to estimate the gradient of $f$ at a particular $x_0.$\n",
        "\n",
        "The $i^{th}$ component of $\\nabla_x f(x_0)$ can be estimated as\n",
        "$$\\frac{f(x_0+\\delta^{i}) - f(x_0-\\delta^{i})}{2\\delta}$$\n",
        "where $\\delta^{i}$ is a column vector whose $i^{th}$ coordinate is\n",
        "$\\delta$, a small constant such as 0.001, and whose other components\n",
        "are zero.\n",
        "Note that adding or subtracting $\\delta^{i}$ is the same as\n",
        "incrementing or decrementing the $i^{th}$ component of $x_0$ by\n",
        "$\\delta$, leaving the other components of $x_0$ unchanged.  Using\n",
        "these results, we can estimate the $i^{th}$ component of the gradient.\n",
        "\n",
        "\n",
        "**For example**, take $x^(0) = (1,2,3)^T$. The gradient $\\nabla_x f(x)$ is a vector of the derivatives of $f(x)$ with respect to each component of $x$, or $\\nabla_x f(x) = (\\frac{df(x)}{dx_1},\\frac{df(x)}{dx_2},\\frac{df(x)}{dx_3})^T$.\n",
        "\n",
        "We can approximate the first component of $\\nabla_x f(x)$ as\n",
        "$$\\frac{f((1,2,3)^T+(0.01,0,0)^T) - f((1,2,3)^T-(0.01,0,0)^T)}{2\\cdot 0.01}.$$\n",
        "\n",
        "(We add the transpose so that these are column vectors.)\n",
        "**This process should be done for each dimension independently,\n",
        "and together the results of each computation are compiled to give the\n",
        "estimated gradient, which is $d$ dimensional.**\n",
        "\n",
        "Implement this as a function `num_grad` that takes as arguments the\n",
        "objective function `f` and a value of `delta`, and returns a new\n",
        "**function** that takes an `x` (a column vector of parameters) and\n",
        "returns a gradient column vector.\n",
        "\n",
        "**Note:** Watch  out for aliasing. If you do temp_x = x where x is a vector (numpy array), then temp_x is just another name for the same vector as x and changing an entry in one will change an entry in the other. You should either use x.copy() or remember to change entries back after modification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPVwGZ-l6XvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def num_grad(f, delta=0.001):\n",
        "    def df(x):\n",
        "        m,n=x.shape\n",
        "        delta_mat = np.zeros((m,n))\n",
        "        grad_vect = np.zeros((m,n))\n",
        "        for i in range(m):\n",
        "            temp_delta = delta_mat.copy()\n",
        "            temp_delta[i,0]=delta\n",
        "            add = x+temp_delta\n",
        "            sub = x-temp_delta\n",
        "            grad_vect[i,0]=(f(add)-f(sub))/0.002\n",
        "        return grad_vect\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kElTR0bL7cbG",
        "colab_type": "text"
      },
      "source": [
        "The test cases are shown below; these use the functions defined in the previous exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiWOdSl_6yAE",
        "colab_type": "code",
        "outputId": "90dc7eb2-37fa-4fd5-9458-6ffce5ad169c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "test_num_grad(num_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1\n",
            "Passed\n",
            "Test 2\n",
            "Passed\n",
            "Test 3\n",
            "Passed\n",
            "Test 4\n",
            "Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WASaSsYu75sG",
        "colab_type": "text"
      },
      "source": [
        "A faster (one function evaluation per entry), though sometimes less\n",
        "accurate, estimate is to use:\n",
        "$$\\frac{f(x_0+\\delta^{i}) - f(x_0)}{\\delta}$$\n",
        "for the $i^{th}$ component of $\\nabla_x f(x_0).$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E31sdqyG78jD",
        "colab_type": "text"
      },
      "source": [
        "### 3.3) Using the Numerical Gradient\n",
        "Recall that our generic gradient descent function takes both a function\n",
        "`f` that returns the value of our function at a given point, and `df`,\n",
        "a function that returns a gradient at a given point.  Write a function\n",
        "`minimize` that takes only a function `f` and uses this function and\n",
        "numerical gradient descent to return the local minimum.  We have\n",
        "provided you with our implementations of `num_grad` and `gd`, so you\n",
        "should not redefine them in the code box below.\n",
        "You may use the default of `delta=0.001` for `num_grad`.\n",
        "\n",
        "**Hint:** Your definition of `minimize` should call `num_grad` exactly\n",
        "once, to return a function that is called many times.\n",
        "You should return the same outputs as `gd`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CStwqDem76Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minimize(f, x0, step_size_fn, num_steps):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      See definitions in part 1\n",
        "    Returns:\n",
        "      same output as gd\n",
        "    \"\"\"\n",
        "    X = x0\n",
        "    func = num_grad(f, delta=0.001)\n",
        "    for i in range(num_steps):\n",
        "        step_size = step_size_fn(i)\n",
        "        X = X- step_size*func(X)\n",
        "    return X,f(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gl0FTby8EQq",
        "colab_type": "text"
      },
      "source": [
        "The test cases are below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxBLWJFm8DnV",
        "colab_type": "code",
        "outputId": "889f5a01-3774-465c-903a-aa9668e92493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_minimize(minimize)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1\n",
            "Passed\n",
            "Test 2\n",
            "Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH-1e98V8LtM",
        "colab_type": "text"
      },
      "source": [
        "## 4) Applying gradient descent to Linear Logistic Classification objective\n",
        "\n",
        "**Note:** In this section,\n",
        "you will code many individual functions, each of which depends on previous ones.\n",
        "We **strongly recommend** that you test each of the components on your own to debug.\n",
        "\n",
        "### 4.1) Calculating the Linear Logistic Classification (LLC) objective\n",
        "\n",
        "First, implement the sigmoid function and implement NLL loss over the data points and separator.\n",
        "Using the latter function, implement the LLC objective.\n",
        "Note that these functions should work for matrix/vector arguments,\n",
        "so that we can compute the objective for a whole dataset with one call.\n",
        "\n",
        "Note that `X` <b>(Upper case X is the dataset here)</b>  is $d \\times n$, `y` is $1 \\times n$, `th` is $d \\times 1$, `th0` is $1 \\times 1$, `lam` is a scalar.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_6E78BF8e2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x is a column vector\n",
        "# returns a vector of the same shape as x\n",
        "def sigmoid(x):\n",
        "    y = np.exp(-x)\n",
        "    z = 1/(1+y)\n",
        "    return z\n",
        "\n",
        "# X is dxn, y is 1xn, th is dx1, th0 is 1x1\n",
        "# returns (1,n) the nll loss for each data point given th and th0 \n",
        "def nll_loss(X, y, th, th0):\n",
        "    z = np.matmul(np.transpose(th),X)+th0\n",
        "    g = sigmoid(z)\n",
        "    return -np.multiply(y,np.log(g))-np.multiply((1-y),np.log(1-g))\n",
        "\n",
        "# X is dxn, y is 1xn, th is dx1, th0 is 1x1, lam is a scalar\n",
        "# returns (float) the llc objective over the dataset\n",
        "def llc_obj(X, y, th, th0, lam):\n",
        "    th0_2mat = np.multiply(th,th)\n",
        "    th0_2mod = np.sum(th0_2mat)\n",
        "    d,n = X.shape \n",
        "    summation_mat = nll_loss(X, y, th, th0)\n",
        "    sum = np.sum(summation_mat)\n",
        "    avg = sum/n\n",
        "    return avg + lam*th0_2mod\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POFvK7zW8iYK",
        "colab_type": "code",
        "outputId": "5b35fc3a-f560-4652-9655-4f6dc6da7701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test_llc_obj(sigmoid,nll_loss,llc_obj)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1 passed\n",
            "Test 2 passed\n",
            "Test 3 passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBB0R4u8tF1",
        "colab_type": "text"
      },
      "source": [
        "### 4.2) Calculating the Linear Logistic Classification gradient\n",
        "\n",
        "Define a function `llc_obj_grad` that returns the gradient of the logistic regression\n",
        "objective function with respect to $\\theta$ and $\\theta_0$ in a single\n",
        "column vector.  The last component of the gradient vector should be\n",
        "the partial derivative with respect to $\\theta_0$.  Look at\n",
        "`np.vstack` as a simple way of stacking two matrices/vectors\n",
        "vertically.  We have broken it down into pieces that mimic steps in\n",
        "the chain rule; this leads to code that is a bit inefficient but\n",
        "easier to write and debug.  We can worry about efficiency later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAtDiGVK8vnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns the gradient of sigmoid with respect to x\n",
        "def d_sigmoid(x):\n",
        "    y = np.exp(-x)\n",
        "    g = 1/(1+y)\n",
        "    g_2 = np.multiply(g,g)\n",
        "    return g-g_2\n",
        "\n",
        "# returns (d,n) the gradient of nll_loss(X, y, th, th0) with respect to th for each data point\n",
        "def d_nll_loss_th(X, y, th, th0):\n",
        "    z = np.matmul(np.transpose(th),X)+th0\n",
        "    g = sigmoid(z) \n",
        "    sub = g-y\n",
        "    return np.multiply(sub,X)\n",
        "\n",
        "# returns (1,n) the gradient of nll_loss(X, y, th, th0) with respect to th0\n",
        "def d_nll_loss_th0(X, y, th, th0):\n",
        "    z = np.matmul(np.transpose(th),X)+th0\n",
        "    g = sigmoid(z) \n",
        "    sub = g-y\n",
        "    return sub\n",
        "\n",
        "# returns (d,1) the gradient of llc_obj(X, y, th, th0) with respect to th\n",
        "def d_llc_obj_th(X, y, th, th0, lam):\n",
        "    sum_vect = 0\n",
        "    z = np.matmul(np.transpose(th),X)+th0\n",
        "    g = sigmoid(z)\n",
        "    d,n = X.shape\n",
        "    print('hi',d,n)\n",
        "    print('di',g.shape)\n",
        "    print('ni',y.shape)\n",
        "    for i in range(n):\n",
        "        sum_vect += (g[0,i]-y[0,i])*X[:,[i]]\n",
        "    avg_sum = sum_vect/n\n",
        "    return avg_sum + 2*lam*th\n",
        "\n",
        "# returns (1,1) the gradient of llc_obj(X, y, th, th0) with respect to th0\n",
        "def d_llc_obj_th0(X, y, th, th0, lam):\n",
        "    z = np.matmul(np.transpose(th),X)+th0\n",
        "    d,n = X.shape\n",
        "    g = sigmoid(z)\n",
        "    sum_g = np.sum(g)\n",
        "    sum_y = np.sum(y)\n",
        "    val = (sum_g-sum_y)/n\n",
        "    return np.array([[val]])\n",
        "\n",
        "# returns (d+1, 1) the full gradient as a single vector (which includes both th, th0)\n",
        "def llc_obj_grad(X, y, th, th0, lam):\n",
        "    th_vect = d_llc_obj_th(X, y, th, th0, lam) \n",
        "    th0_vect = d_llc_obj_th0(X, y, th, th0, lam) \n",
        "    return np.vstack((th_vect,th0_vect))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDP6H_2P80vm",
        "colab_type": "text"
      },
      "source": [
        "Some test cases that may be of use are provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNuF6-c38yji",
        "colab_type": "code",
        "outputId": "0496b2b6-c57d-47f0-af09-e13840a19afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "test_llc_grad(d_sigmoid,d_nll_loss_th,d_nll_loss_th0,d_llc_obj_th,d_llc_obj_th0,llc_obj_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1 passed\n",
            "Test 2 passed\n",
            "Test 3 passed\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "Test 4 passed\n",
            "Test 5 passed\n",
            "Test 6 passed\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "Test 7 passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vf6OFEU89pC",
        "colab_type": "text"
      },
      "source": [
        "### 4.3) Linear Logistic Classification minimize\n",
        "\n",
        "Putting it all together, use the functions you built earlier to write\n",
        "a gradient descent minimizer for the LLC objective.  You do not need\n",
        "to paste in your previous definitions; you can just call the ones\n",
        "you've defined above.  You will need to call `gd`; your function `llc_min` should return\n",
        "the values that `gd` does.\n",
        "\n",
        "* Initialize all the separator parameters to zero,\n",
        "* use the step size function provided below, and\n",
        "* specify 10 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIqWIYnq8_Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def llc_min(data, labels, lam):\n",
        "    \n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        data: dxn\n",
        "        labels: 1xn\n",
        "        lam: scalar\n",
        "    Returns:\n",
        "        same output as gd\n",
        "    \"\"\"\n",
        "    def lil_min_step_size_fn(i):\n",
        "        return 2/(i+1)**0.5\n",
        "    d,n = data.shape\n",
        "    theta=np.zeros((d+1,1))\n",
        "    def f(theta):\n",
        "        return llc_obj(data, labels, theta[:d,[0]], theta[d,0], lam)\n",
        "    def df(theta):\n",
        "        return llc_obj_grad(data, labels,theta[:d,[0]], theta[d,0] , lam) \n",
        "    return gd(f,df,theta,lil_min_step_size_fn,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH4xd7C-9BIm",
        "colab_type": "text"
      },
      "source": [
        "Test cases are shown below, where an additional separable test\n",
        "data set has been specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgOC_i879Acd",
        "colab_type": "code",
        "outputId": "c2e91a64-bf31-4464-84ae-89744df82e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_llc_min(llc_min)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "Test 1 passed\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "hi 2 4\n",
            "di (1, 4)\n",
            "ni (1, 4)\n",
            "Test 2 passed\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}